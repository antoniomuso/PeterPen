{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "import random\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/pad/lines_pad.json', 'r') as f:\n",
    "    data_lines = json.load(f)\n",
    "\n",
    "with open('../dati/pad/lines_2_pad.json', 'r') as f:\n",
    "    data_lines += json.load(f)\n",
    "\n",
    "for elem in range(len(data_lines)):\n",
    "    for arr in range(len(data_lines[elem])):\n",
    "        tmp = []\n",
    "        for f in range(7):\n",
    "            tmp.append(data_lines[elem][arr][f])\n",
    "        data_lines[elem][arr] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/pad/circles_pad.json', 'r') as f:\n",
    "    data_circles = json.load(f)\n",
    "\n",
    "for elem in range(len(data_circles)):\n",
    "    for arr in range(len(data_circles[elem])):\n",
    "        tmp = []\n",
    "        for f in range(7):\n",
    "            tmp.append(data_circles[elem][arr][f])\n",
    "        data_circles[elem][arr] = tmp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/pad/lines_pad.json', 'r') as f:\n",
    "    data_lines = json.load(f)\n",
    "\n",
    "with open('../dati/pad/lines_2_pad.json', 'r') as f:\n",
    "    data_lines += json.load(f)\n",
    "    \n",
    "\n",
    "def generator(data, labels):\n",
    "    assert len(data) == len(labels)\n",
    "    while True:\n",
    "        for elem in range(len(data)):\n",
    "            #word_array = []\n",
    "            #for arr in range(len(data[elem])):\n",
    "            #    tmp = []\n",
    "            #    for f in range(7):\n",
    "            #        tmp.append(data[elem][arr][f])\n",
    "            #    word_array.append(tmp)\n",
    "            yield np.array(data[elem]), np.array(labels[elem])\n",
    "\n",
    "g_lines = generator(data_lines, [1] * len(data_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/pad/circles_pad.json', 'r') as f:\n",
    "    data_circles = json.load(f)\n",
    "    \n",
    "g_circles = generator(data_circles, [0] * len(data_circles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/dati_con_penna/concatenati/pad/dario_1_concat_dario_2_pad.json', 'r') as f:\n",
    "    data_d = json.load(f)\n",
    "    \n",
    "g_dario = generator(data_d[:-3], [1] * 7)\n",
    "g_dario_test = generator(data_d[-3:], [1] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/dati_con_penna/concatenati/pad/antonio_1_concat_antonio_2_pad.json', 'r') as f:\n",
    "    data_a = json.load(f)\n",
    "    \n",
    "g_antonio = generator(data_a[:-3], [0] * 8)\n",
    "g_antonio_test = generator(data_a[-3:], [0] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/dati_con_penna/concatenati/pad/manuel_1_concat_manuel_2_pad.json', 'r') as f:\n",
    "    data_t = json.load(f)\n",
    "    \n",
    "g_taraz = generator(data_t[:-3], [0] * 7)\n",
    "g_taraz_test = generator(data_t[-3:], [0] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/dati_con_penna/concatenati/pad/giovanni_1_concat_giovanni_2_pad.json', 'r') as f:\n",
    "    data_g = json.load(f)\n",
    "    \n",
    "g_giovanni = generator(data_g[:-3], [0] * 5)\n",
    "g_giovanni_test = generator(data_g[-3:], [0] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data_d, labelled_data_g = [], []\n",
    "for i in range(len(data_d)):\n",
    "    labelled_data_d.append((data_d[i], 1))\n",
    "\n",
    "for i in range(len(data_g)):\n",
    "    labelled_data_g.append((data_g[i], 0))\n",
    "    \n",
    "for i in range(len(data_a)):\n",
    "    labelled_data_g.append((data_a[i], 0))\n",
    "\n",
    "for i in range(len(data_t)):\n",
    "    labelled_data_g.append((data_t[i], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data_dg = labelled_data_d + labelled_data_g  \n",
    "random.shuffle(labelled_data_dg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 600, 7)\n"
     ]
    }
   ],
   "source": [
    "def xy_data(labelled_data):\n",
    "    x_data, y_labels = [], []\n",
    "    for i in labelled_data:\n",
    "        x_data.append(i[0])\n",
    "        y_labels.append(i[1])\n",
    "        \n",
    "    return np.array(x_data), np.array(y_labels)\n",
    "\n",
    "data_dg, label_dg = xy_data(labelled_data_dg)\n",
    "print(data_dg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_generator(g1, g2, batch_size):\n",
    "    while True:\n",
    "        batch = []\n",
    "        label = []\n",
    "        for _ in range(batch_size // 2):\n",
    "            tmp = next(g1)\n",
    "            batch.append(tmp[0])\n",
    "            label.append(tmp[1])\n",
    "            tmp = next(g2)\n",
    "            batch.append(tmp[0])\n",
    "            label.append(tmp[1])\n",
    "        print(np.array(label).shape)\n",
    "        yield np.array(batch), np.array(label)\n",
    "\n",
    "g_mix = mix_generator(g_lines, g_circles, 10)\n",
    "g_dario_giovanni = mix_generator(g_dario, g_giovanni, 10)\n",
    "g_dario_antonio = mix_generator(g_dario, g_antonio, 10)\n",
    "g_dario_taraz = mix_generator(g_dario, g_taraz, 10)\n",
    "g_dario_giovanni_antonio = mix_generator(g_dario_giovanni, g_dario_antonio, 10)\n",
    "g_dario_tutti = mix_generator(g_dario_giovanni_antonio, g_dario_taraz, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_one_against_all(path_genuine, paths_impostors, batch_size):\n",
    "    with open(path_genuine, 'r') as f:\n",
    "        genuine_data = json.load(f)\n",
    "        \n",
    "    train_ratio = round(len(genuine_data) / 5)\n",
    "    assert len(genuine_data[:-train_ratio]) + len(genuine_data[-train_ratio:]) == len(genuine_data)\n",
    "    genuine_generator = generator(genuine_data[:-train_ratio], [1] * (len(genuine_data) - train_ratio))\n",
    "    genuine_generator_test = generator(genuine_data[-train_ratio:], [1] * train_ratio)\n",
    "    \n",
    "    generator_all = genuine_generator\n",
    "    generator_all_test = genuine_generator_test\n",
    "    \n",
    "    for path in paths_impostors:\n",
    "        with open(path, 'r') as f:\n",
    "            impostor_data = json.load(f)\n",
    "        train_ratio = round(len(impostor_data) / 5)\n",
    "        assert len(impostor_data[:-train_ratio]) + len(impostor_data[-train_ratio:]) == len(impostor_data)\n",
    "        generator_all = mix_generator(generator_all, generator(impostor_data[:-train_ratio], [0] * (len(impostor_data) - train_ratio)), batch_size)\n",
    "        generator_all_test = mix_generator(generator_all_test, generator(impostor_data[-train_ratio:], [0] * train_ratio), batch_size)\n",
    "        \n",
    "    return generator_all, generator_all_test\n",
    "\n",
    "impostors = ['../dati/pad/giovanni_pad.json', '../dati/pad/taraz_pad.json']\n",
    "g_dario_impostors, g_dario_impostors_test = generator_one_against_all('../dati/pad/dario_pad.json', impostors, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dario = Sequential()\n",
    "model_dario.add(LSTM(input_shape=(600, 7), units=200, activation=\"sigmoid\", return_sequences=True, recurrent_activation=\"hard_sigmoid\"))\n",
    "model_dario.add(LSTM(units=200, activation=\"sigmoid\", return_sequences=False, recurrent_activation=\"hard_sigmoid\"))\n",
    "model_dario.add(Dropout(0.5))\n",
    "model_dario.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dario.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_dario, show_shapes=True, to_file='lstm_dropout_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist = model_dario.fit_generator(g_dario_impostors, epochs=5, steps_per_epoch=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 131 samples, validate on 65 samples\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can only use `validation_steps` when doing step-wise training, i.e. `steps_per_epoch` must be set.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ceb1fb7d46cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_dario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_dg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             raise ValueError('Can only use `validation_steps` '\n\u001b[0m\u001b[1;32m     78\u001b[0m                              \u001b[0;34m'when doing step-wise '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                              \u001b[0;34m'training, i.e. `steps_per_epoch` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can only use `validation_steps` when doing step-wise training, i.e. `steps_per_epoch` must be set."
     ]
    }
   ],
   "source": [
    "hist = model_dario.fit(x = data_dg, y = label_dg, epochs=10, steps_per_epoch=131, validation_split=0.33, validation_steps=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dario = model_dario.evaluate_generator(g_dario_test, verbose=1, steps=3)\n",
    "out_giovanni = model_dario.evaluate_generator(g_giovanni_test, verbose=1, steps=3)\n",
    "out_taraz = model_dario.evaluate_generator(g_taraz_test, verbose=1, steps=3)\n",
    "\n",
    "out_dario, out_giovanni, out_taraz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dario.save(\"model_dario.h5\")\n",
    "del model_dario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dario_reloaded = load_model(\"model_dario.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dario_reloaded = model_dario_reloaded.evaluate_generator(g_dario_test, verbose=1, steps=3)\n",
    "\n",
    "out_dario_reloaded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
