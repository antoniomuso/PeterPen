{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "import random\n",
    "from keras.layers import Dense, LSTM, Dropout, Masking\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/pad/lines_pad.json', 'r') as f:\n",
    "    data_lines = json.load(f)\n",
    "\n",
    "with open('../dati/pad/lines_2_pad.json', 'r') as f:\n",
    "    data_lines += json.load(f)\n",
    "\n",
    "for elem in range(len(data_lines)):\n",
    "    for arr in range(len(data_lines[elem])):\n",
    "        tmp = []\n",
    "        for f in range(7):\n",
    "            tmp.append(data_lines[elem][arr][f])\n",
    "        data_lines[elem][arr] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/pad/circles_pad.json', 'r') as f:\n",
    "    data_circles = json.load(f)\n",
    "\n",
    "for elem in range(len(data_circles)):\n",
    "    for arr in range(len(data_circles[elem])):\n",
    "        tmp = []\n",
    "        for f in range(7):\n",
    "            tmp.append(data_circles[elem][arr][f])\n",
    "        data_circles[elem][arr] = tmp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/pad/lines_pad.json', 'r') as f:\n",
    "    data_lines = json.load(f)\n",
    "\n",
    "with open('../dati/pad/lines_2_pad.json', 'r') as f:\n",
    "    data_lines += json.load(f)\n",
    "    \n",
    "\n",
    "def generator(data, labels):\n",
    "    assert len(data) == len(labels)\n",
    "    while True:\n",
    "        for elem in range(len(data)):\n",
    "            #word_array = []\n",
    "            #for arr in range(len(data[elem])):\n",
    "            #    tmp = []\n",
    "            #    for f in range(7):\n",
    "            #        tmp.append(data[elem][arr][f])\n",
    "            #    word_array.append(tmp)\n",
    "            yield np.array(data[elem]), np.array(labels[elem])\n",
    "\n",
    "g_lines = generator(data_lines, [1] * len(data_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dati/pad/circles_pad.json', 'r') as f:\n",
    "    data_circles = json.load(f)\n",
    "    \n",
    "g_circles = generator(data_circles, [0] * len(data_circles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_recog = 'Antonio_pad_concat_antonio_recognition_pad.json'\n",
    "file_path = '../dati/scrittura_di_computer/pad/'\n",
    "labelled_data_d, labelled_data_g = [], []\n",
    "\n",
    "for file in os.listdir(file_path):\n",
    "    if os.path.splitext(file)[1] != '.json': continue\n",
    "    if file_recog == file: continue\n",
    "    with open(os.path.join(file_path, file), 'r') as f:\n",
    "        data_g = json.load(f)\n",
    "    for i in range(len(data_g)):\n",
    "        labelled_data_g.append((data_g[i], 0))\n",
    "        \n",
    "        \n",
    "with open(os.path.join(file_path, file_recog), 'r') as f:\n",
    "    data_r = json.load(f)\n",
    "for i in range(len(data_r)):\n",
    "    labelled_data_d.append((data_r[i], 1))\n",
    "    \n",
    "def xy_data(labelled_data):\n",
    "    x_data, y_labels = [], []\n",
    "    for i in labelled_data:\n",
    "        x_data.append(i[0])\n",
    "        y_labels.append(i[1])\n",
    "        \n",
    "    return np.array(x_data), np.array(y_labels)\n",
    "\n",
    "labelled_data_dg = labelled_data_d + labelled_data_g  \n",
    "random.shuffle(labelled_data_dg)\n",
    "\n",
    "data_dg, label_dg = xy_data(labelled_data_dg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=0.0))\n",
    "    model.add(LSTM(input_shape=(1000, 7), units=64, activation=\"sigmoid\", return_sequences=True, recurrent_activation=\"hard_sigmoid\"))\n",
    "    model.add(LSTM(units=128, activation=\"sigmoid\", return_sequences=False, recurrent_activation=\"hard_sigmoid\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural_network = KerasClassifier(build_fn=create_model, \n",
    "#                                 epochs=4,\n",
    "#                                 steps_per_epoch=140,\n",
    "#                                 validation_split=0.2,\n",
    "#                                 validation_steps=36,\n",
    "#                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model_dario, show_shapes=True, to_file='lstm_dropout_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist = model_dario.fit_generator(g_dario_impostors, epochs=5, steps_per_epoch=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 558 samples, validate on 276 samples\n",
      "Epoch 1/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.5082 - acc: 0.7885 - val_loss: 0.3716 - val_acc: 0.8732\n",
      "Epoch 2/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.4587 - acc: 0.8297 - val_loss: 0.3443 - val_acc: 0.8732\n",
      "Epoch 3/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.4262 - acc: 0.8297 - val_loss: 0.3301 - val_acc: 0.8732\n",
      "Epoch 4/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.3788 - acc: 0.8280 - val_loss: 0.2954 - val_acc: 0.8732\n",
      "Epoch 5/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.3357 - acc: 0.8441 - val_loss: 0.3054 - val_acc: 0.8732\n",
      "Epoch 6/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.3074 - acc: 0.8602 - val_loss: 0.2191 - val_acc: 0.9348\n",
      "Epoch 7/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.2660 - acc: 0.9014 - val_loss: 0.1985 - val_acc: 0.9420\n",
      "Epoch 8/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.2482 - acc: 0.8996 - val_loss: 0.1803 - val_acc: 0.9420\n",
      "Epoch 9/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.2484 - acc: 0.9032 - val_loss: 0.2049 - val_acc: 0.9348\n",
      "Epoch 10/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.2089 - acc: 0.9229 - val_loss: 0.1526 - val_acc: 0.9529\n",
      "Epoch 11/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.1973 - acc: 0.9211 - val_loss: 0.1524 - val_acc: 0.9457\n",
      "Epoch 12/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.1840 - acc: 0.9427 - val_loss: 0.2570 - val_acc: 0.8913\n",
      "Epoch 13/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.1687 - acc: 0.9337 - val_loss: 0.1328 - val_acc: 0.9493\n",
      "Epoch 14/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.1836 - acc: 0.9211 - val_loss: 0.1929 - val_acc: 0.9275\n",
      "Epoch 15/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.1557 - acc: 0.9427 - val_loss: 0.2912 - val_acc: 0.9094\n",
      "Epoch 16/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.1691 - acc: 0.9373 - val_loss: 0.1705 - val_acc: 0.9384\n",
      "Epoch 17/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.1366 - acc: 0.9588 - val_loss: 0.1101 - val_acc: 0.9565\n",
      "Epoch 18/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.1219 - acc: 0.9534 - val_loss: 0.1128 - val_acc: 0.9493\n",
      "Epoch 19/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.1720 - acc: 0.9247 - val_loss: 0.1298 - val_acc: 0.9493\n",
      "Epoch 20/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.1760 - acc: 0.9301 - val_loss: 0.0959 - val_acc: 0.9638\n",
      "Epoch 21/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.1380 - acc: 0.9355 - val_loss: 0.0916 - val_acc: 0.9638\n",
      "Epoch 22/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.1237 - acc: 0.9570 - val_loss: 0.0837 - val_acc: 0.9674\n",
      "Epoch 23/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0903 - acc: 0.9713 - val_loss: 0.1063 - val_acc: 0.9565\n",
      "Epoch 24/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.0676 - acc: 0.9821 - val_loss: 0.3209 - val_acc: 0.8986\n",
      "Epoch 25/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.1161 - acc: 0.9588 - val_loss: 0.0771 - val_acc: 0.9601\n",
      "Epoch 26/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.1381 - acc: 0.9444 - val_loss: 0.1521 - val_acc: 0.9384\n",
      "Epoch 27/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0557 - acc: 0.9803 - val_loss: 0.0638 - val_acc: 0.9783\n",
      "Epoch 28/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.1198 - acc: 0.9373 - val_loss: 0.0737 - val_acc: 0.9638\n",
      "Epoch 29/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0710 - acc: 0.9767 - val_loss: 0.0834 - val_acc: 0.9638\n",
      "Epoch 30/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.1411 - acc: 0.9373 - val_loss: 0.0563 - val_acc: 0.9819\n",
      "Epoch 31/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.0479 - acc: 0.9821 - val_loss: 0.0435 - val_acc: 0.9819\n",
      "Epoch 32/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.1289 - acc: 0.9570 - val_loss: 0.0403 - val_acc: 0.9819\n",
      "Epoch 33/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0476 - acc: 0.9857 - val_loss: 0.0473 - val_acc: 0.9819\n",
      "Epoch 34/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.0987 - acc: 0.9677 - val_loss: 0.0366 - val_acc: 0.9891\n",
      "Epoch 35/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.1167 - acc: 0.9731 - val_loss: 0.2041 - val_acc: 0.9203\n",
      "Epoch 36/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0612 - acc: 0.9821 - val_loss: 0.0783 - val_acc: 0.9710\n",
      "Epoch 37/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.0380 - acc: 0.9857 - val_loss: 0.0296 - val_acc: 0.9855\n",
      "Epoch 38/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0463 - acc: 0.9875 - val_loss: 0.5322 - val_acc: 0.8732\n",
      "Epoch 39/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0764 - acc: 0.9767 - val_loss: 0.0284 - val_acc: 0.9928\n",
      "Epoch 40/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.1097 - acc: 0.9642 - val_loss: 0.0268 - val_acc: 0.9891\n",
      "Epoch 41/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0379 - acc: 0.9875 - val_loss: 0.0286 - val_acc: 0.9928\n",
      "Epoch 42/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0730 - acc: 0.9803 - val_loss: 0.1124 - val_acc: 0.9529\n",
      "Epoch 43/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.0181 - acc: 0.9946 - val_loss: 0.0156 - val_acc: 0.9928\n",
      "Epoch 44/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.0480 - acc: 0.9875 - val_loss: 0.0278 - val_acc: 0.9855\n",
      "Epoch 45/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0793 - acc: 0.9659 - val_loss: 0.0847 - val_acc: 0.9601\n",
      "Epoch 46/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0146 - acc: 0.9946 - val_loss: 0.0087 - val_acc: 1.0000\n",
      "Epoch 47/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 48/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.1097 - acc: 0.9516 - val_loss: 0.0298 - val_acc: 0.9891\n",
      "Epoch 49/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.1340 - val_acc: 0.9420\n",
      "Epoch 50/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0239 - acc: 0.9928 - val_loss: 0.0149 - val_acc: 0.9928\n",
      "Epoch 51/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0868 - acc: 0.9821 - val_loss: 0.0278 - val_acc: 0.9891\n",
      "Epoch 52/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 53/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.1192 - acc: 0.9659 - val_loss: 0.0212 - val_acc: 0.9928\n",
      "Epoch 54/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0064 - val_acc: 1.0000\n",
      "Epoch 55/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 56/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0837 - acc: 0.9803 - val_loss: 0.0204 - val_acc: 0.9928\n",
      "Epoch 57/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0084 - val_acc: 0.9964\n",
      "Epoch 58/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 59/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.1108 - acc: 0.9749 - val_loss: 0.0115 - val_acc: 0.9964\n",
      "Epoch 60/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.0155 - val_acc: 0.9928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0124 - val_acc: 0.9891\n",
      "Epoch 62/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5731 - val_acc: 0.8804\n",
      "Epoch 63/110\n",
      "558/558 [==============================] - 34s 60ms/step - loss: 0.2551 - acc: 0.9391 - val_loss: 0.0156 - val_acc: 0.9964\n",
      "Epoch 64/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 0.9964\n",
      "Epoch 65/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 66/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 8.9206e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 67/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.1746 - acc: 0.9624 - val_loss: 0.1178 - val_acc: 0.9529\n",
      "Epoch 68/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0192 - acc: 0.9946 - val_loss: 0.0132 - val_acc: 0.9964\n",
      "Epoch 69/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 70/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 6.9295e-04 - acc: 1.0000 - val_loss: 9.5572e-04 - val_acc: 1.0000\n",
      "Epoch 71/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 4.9084e-04 - acc: 1.0000 - val_loss: 6.9925e-04 - val_acc: 1.0000\n",
      "Epoch 72/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.1491 - acc: 0.9695 - val_loss: 0.0209 - val_acc: 0.9928\n",
      "Epoch 73/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.0082 - acc: 0.9982 - val_loss: 0.0446 - val_acc: 0.9783\n",
      "Epoch 74/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0428 - acc: 0.9875 - val_loss: 0.0111 - val_acc: 0.9964\n",
      "Epoch 75/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 1.0000\n",
      "Epoch 76/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 6.5916e-04 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 1.0000\n",
      "Epoch 77/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.0609 - acc: 0.9803 - val_loss: 0.0175 - val_acc: 0.9964\n",
      "Epoch 78/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.0774 - acc: 0.9785 - val_loss: 0.0207 - val_acc: 0.9891\n",
      "Epoch 79/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0048 - val_acc: 1.0000\n",
      "Epoch 80/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 81/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 5.6811e-04 - acc: 1.0000 - val_loss: 8.1882e-04 - val_acc: 1.0000\n",
      "Epoch 82/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 0.2302 - acc: 0.9552 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "Epoch 83/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 7.0900e-04 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 1.0000\n",
      "Epoch 84/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 5.4849e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 1.0000\n",
      "Epoch 85/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 4.8138e-04 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 1.0000\n",
      "Epoch 86/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.0374 - acc: 0.9910 - val_loss: 0.0068 - val_acc: 1.0000\n",
      "Epoch 87/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0023 - acc: 0.9982 - val_loss: 7.3612e-04 - val_acc: 1.0000\n",
      "Epoch 88/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.1417 - acc: 0.9713 - val_loss: 0.0408 - val_acc: 0.9855\n",
      "Epoch 89/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.0064 - acc: 0.9982 - val_loss: 0.0188 - val_acc: 0.9928\n",
      "Epoch 90/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 91/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 4.7619e-04 - acc: 1.0000 - val_loss: 6.6448e-04 - val_acc: 1.0000\n",
      "Epoch 92/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 3.1764e-04 - acc: 1.0000 - val_loss: 0.0104 - val_acc: 0.9964\n",
      "Epoch 93/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.1309 - acc: 0.9695 - val_loss: 0.0074 - val_acc: 0.9964\n",
      "Epoch 94/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 1.0000\n",
      "Epoch 95/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 4.1851e-04 - acc: 1.0000 - val_loss: 4.5300e-04 - val_acc: 1.0000\n",
      "Epoch 96/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 1.8750e-04 - acc: 1.0000 - val_loss: 2.7110e-04 - val_acc: 1.0000\n",
      "Epoch 97/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.0835 - acc: 0.9731 - val_loss: 0.0348 - val_acc: 0.9891\n",
      "Epoch 98/110\n",
      "558/558 [==============================] - 35s 62ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0099 - val_acc: 0.9964\n",
      "Epoch 99/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 9.9104e-04 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 1.0000\n",
      "Epoch 100/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 3.8763e-04 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 0.9964\n",
      "Epoch 101/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 0.0608 - acc: 0.9839 - val_loss: 0.0935 - val_acc: 0.9601\n",
      "Epoch 102/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0149 - acc: 0.9982 - val_loss: 0.0047 - val_acc: 1.0000\n",
      "Epoch 103/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 104/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 3.0425e-04 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 1.0000\n",
      "Epoch 105/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 2.8914e-04 - acc: 1.0000 - val_loss: 6.2684e-04 - val_acc: 1.0000\n",
      "Epoch 106/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 0.0735 - acc: 0.9821 - val_loss: 0.0258 - val_acc: 0.9855\n",
      "Epoch 107/110\n",
      "558/558 [==============================] - 34s 61ms/step - loss: 9.8022e-04 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 108/110\n",
      "558/558 [==============================] - 34s 62ms/step - loss: 4.8030e-04 - acc: 1.0000 - val_loss: 0.0012 - val_acc: 1.0000\n",
      "Epoch 109/110\n",
      "558/558 [==============================] - 35s 63ms/step - loss: 2.5877e-04 - acc: 1.0000 - val_loss: 2.7240e-04 - val_acc: 1.0000\n",
      "Epoch 110/110\n",
      "160/558 [=======>......................] - ETA: 20s - loss: 2.2707e-04 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "cb = keras.callbacks.TensorBoard(log_dir='/usr/Graph', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "hist = model.fit(x = data_dg, y = label_dg, epochs=110, batch_size=32, callbacks=[cb],validation_split=0.33)\n",
    "\n",
    "#out = cross_val_score(neural_network, data_dg, label_dg, cv=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.save('model_antonio_good_performance_65_instanze.h5')\n",
    "#out\n",
    "#!git config --global user.email \"antoniomusolino007@gmail.com\"\n",
    "#!git stash --include-untracked\n",
    "!git add ../.\n",
    "!git commit -m 'nuove curve'\n",
    "!git pull\n",
    "#!git merge\n",
    "!git push\n",
    "#print(out)\n",
    "#out.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gen_mean (curve):\n",
    "    mean = [0]* len(curve)\n",
    "    mean[0] = curve[0]\n",
    "    for i in range(1,len(curve)):\n",
    "        mean[i] = ((mean[i-1] )*i + out[i])/(i+1)\n",
    "    return mean\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "\n",
    "\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('Model_Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Learn', 'Test'], loc='lower right')\n",
    "plt.savefig('./150_epoch_accuracy_new_data_Manuel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history['acc'])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('./model_accuracy_40Epoch_scaled',quality=100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#model.save('model_giulio_good_performance_23.h5')\n",
    "#plt.savefig('./model_loss_40Epoch_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_manuel = load_model('model_manuel_good_performance.h5')\n",
    "model_giovanni = load_model('model_giovanni_good_performance.h5')\n",
    "model_dario_reloaded = load_model(\"model_dario_good_performance.h5\")\n",
    "model_antonio_reloaded = load_model('model_antonio_good_performance_65_instanze.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = xy_data(labelled_data_g)\n",
    "x1, y1, = xy_data(labelled_data_d)\n",
    "out_dario_reloaded = model_dario_reloaded.evaluate(x, y)\n",
    "out_dario_reloaded2 = model_dario_reloaded.evaluate(x1, y1)\n",
    "\n",
    "print(out_dario_reloaded)\n",
    "print(out_dario_reloaded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76, 1000, 7)\n",
      "76/76 [==============================] - 2s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06141953983981358, 0.9868421084002444]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join('../dati/scrittura_di_computer/' ,'antonio_76_volte_pad.json'), 'r') as f:\n",
    "    check = json.load(f)\n",
    "\n",
    "check = np.array(check)\n",
    "print(check.shape)\n",
    "\n",
    "model_antonio_reloaded.evaluate(check, np.ones(check.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.84370333e-06, 2.08774509e-06, 3.75387299e-06, 1.39952829e-06,\n",
       "       2.26093857e-06, 1.79741055e-05, 2.83092504e-06, 7.37263804e-07,\n",
       "       6.44855709e-06, 7.52337519e-06, 2.48218203e-05, 7.19082948e-07,\n",
       "       2.14701527e-06, 5.64987249e-06, 1.39370441e-05, 7.65337536e-05,\n",
       "       1.02461154e-05, 2.84167118e-06, 2.06801019e-06, 8.46866169e-06,\n",
       "       3.29693557e-06, 1.73625938e-06, 5.62416062e-06, 2.34268941e-06,\n",
       "       9.99145269e-01, 1.33383810e-06, 1.80017414e-05, 3.25551241e-06,\n",
       "       1.53868541e-06, 1.44932210e-06, 8.05695527e-06, 6.08462187e-06,\n",
       "       2.99236926e-05, 9.35875141e-06, 2.96213402e-04, 6.79248187e-05,\n",
       "       1.45282456e-06, 9.99856114e-01, 1.84447185e-06, 1.99272326e-05,\n",
       "       2.84901421e-06, 1.28378838e-06, 7.00373846e-07, 1.29499458e-05,\n",
       "       4.76060632e-06, 4.49932922e-05, 9.99928236e-01, 1.68307474e-06,\n",
       "       2.70249893e-06, 3.79845892e-06, 9.57761245e-07, 9.99900460e-01,\n",
       "       9.50875801e-06, 4.71382100e-06, 3.56635996e-06, 2.36690471e-06,\n",
       "       2.81125290e-06, 1.31171328e-06, 1.19552180e-06, 1.10172268e-05,\n",
       "       1.41269129e-05, 4.08281812e-05, 6.78885158e-07, 2.41625207e-06,\n",
       "       9.99825060e-01, 1.37400707e-06, 3.01037244e-06, 7.12895826e-06,\n",
       "       1.43658917e-06, 5.69892791e-06, 9.80968480e-07, 2.28238196e-06,\n",
       "       1.37332652e-04, 1.33408366e-06, 7.29255189e-05, 3.06806373e-06,\n",
       "       2.09586824e-06, 2.41632370e-06, 1.81320763e-06, 4.71059611e-04,\n",
       "       4.25210737e-06, 7.03654268e-06, 2.92792743e-06, 1.56399710e-06,\n",
       "       1.83685700e-06, 1.54116731e-06, 1.48896679e-05, 4.54384644e-06,\n",
       "       1.02454782e-04, 9.99876261e-01, 8.56310805e-07, 1.73324679e-06,\n",
       "       1.25216229e-05, 1.67406711e-06, 1.24288135e-05, 1.51568736e-06,\n",
       "       9.19178035e-07, 7.67826407e-07, 2.56617932e-06, 1.08497397e-06,\n",
       "       2.84898147e-06, 9.99495506e-01, 9.99923110e-01, 7.32966055e-06,\n",
       "       2.92084064e-06, 2.52607651e-06, 8.36902254e-05, 5.92217202e-06,\n",
       "       2.40324007e-05, 5.87430433e-04, 3.16113947e-05, 2.64256323e-06,\n",
       "       6.12203667e-06, 9.99860525e-01, 1.73736444e-05, 3.55915336e-06,\n",
       "       1.41478249e-05, 2.11520319e-06, 5.76858583e-05, 5.53934915e-05,\n",
       "       9.11776988e-06, 3.48840990e-06, 3.34774359e-06, 3.70578596e-06,\n",
       "       8.84515794e-06, 1.02698687e-06, 9.99936819e-01, 2.37670110e-06,\n",
       "       8.79702202e-06, 7.66009634e-07, 1.83617738e-06, 1.17259401e-06,\n",
       "       1.95315079e-06, 4.87615034e-06, 3.52863481e-06, 2.91656561e-05,\n",
       "       2.10547432e-06, 2.87251878e-05, 7.30099464e-06, 2.15037585e-06,\n",
       "       4.18047193e-06, 5.69358235e-06, 2.02590240e-06, 7.12363089e-07,\n",
       "       2.25140338e-05, 9.99823153e-01, 9.99938369e-01, 9.27537167e-06,\n",
       "       8.74671116e-07, 9.09733387e-07, 6.67039349e-05, 1.93569758e-06,\n",
       "       1.22916208e-06, 9.99945641e-01, 3.77172933e-06, 1.90790615e-06,\n",
       "       9.99920249e-01, 1.00922693e-06, 1.35575283e-06, 2.87106900e-06,\n",
       "       8.34536536e-07, 9.99819458e-01, 3.25855649e-06, 1.03999878e-06,\n",
       "       1.77094116e-05, 3.02642320e-06, 1.28450847e-06, 1.45465028e-05,\n",
       "       2.30110777e-06, 9.98980343e-01, 9.91968818e-07, 6.56866323e-05,\n",
       "       3.79960011e-06, 1.19081233e-05, 1.29701630e-06, 6.84193492e-06,\n",
       "       2.13410340e-06, 1.61201697e-05, 8.96746371e-07, 9.40739767e-07,\n",
       "       2.28534802e-04, 1.81526650e-06, 4.77151025e-06, 4.69939050e-06,\n",
       "       1.45892791e-05, 1.32612388e-06, 7.06708136e-07, 9.99864578e-01,\n",
       "       1.01928151e-06, 1.47533035e-06, 1.41895612e-06, 3.41663485e-06,\n",
       "       1.29682951e-06, 9.99924660e-01, 9.99929428e-01, 5.12979886e-06,\n",
       "       2.10600047e-06, 2.28206181e-06, 2.21815367e-06, 2.71442150e-05,\n",
       "       1.90652663e-05, 1.85470071e-05, 6.62151706e-06, 7.24950837e-07,\n",
       "       2.97845891e-05, 3.08406161e-05, 1.03788316e-05, 9.41796259e-07,\n",
       "       1.54094823e-06, 3.12187558e-06, 3.44783143e-06, 1.30240812e-06,\n",
       "       5.90229183e-06, 1.33626861e-06, 1.24719872e-06, 3.67051530e-06,\n",
       "       5.40799692e-06, 4.88734213e-06, 1.79670531e-06, 1.76372578e-06,\n",
       "       7.34811647e-06, 1.51720160e-05, 2.12504165e-06, 4.56217240e-05,\n",
       "       6.60321666e-06, 3.34659467e-06, 1.15504781e-06, 1.12625185e-06,\n",
       "       1.12535315e-06, 8.00760972e-07, 9.99927878e-01, 3.70765447e-05,\n",
       "       2.74143895e-06, 8.97375969e-07, 1.08448903e-05, 1.81611847e-06,\n",
       "       2.97833799e-06, 7.59901354e-07, 1.88604895e-06, 9.99774039e-01,\n",
       "       9.99921322e-01, 2.01211037e-06, 1.80876737e-06, 1.13052165e-05,\n",
       "       1.15136572e-06, 1.56378810e-05, 2.18924220e-06, 2.32239995e-06,\n",
       "       1.32204786e-05, 1.38646033e-06, 1.64015419e-05, 1.29090301e-06,\n",
       "       8.19273680e-07, 2.04971684e-06, 3.47849027e-06, 1.40856525e-06,\n",
       "       1.43218836e-06, 1.44726539e-06, 2.20474681e-06, 1.86651289e-06,\n",
       "       4.46884724e-06, 1.68418910e-06, 4.72842657e-06, 2.32562609e-03,\n",
       "       4.11836285e-04, 9.99797523e-01, 4.45446349e-05, 1.10441661e-05,\n",
       "       1.01738192e-06, 4.55530108e-06, 1.02923809e-06, 1.79654069e-06,\n",
       "       9.99726951e-01, 5.11449980e-06, 2.56532876e-05, 8.08329276e-07,\n",
       "       9.39025085e-07, 2.82149676e-06, 1.04260448e-06, 2.67720293e-06,\n",
       "       6.63421497e-06, 7.27913630e-06, 2.23324946e-06, 6.49983349e-06,\n",
       "       1.74228376e-06, 9.99817789e-01, 9.35935168e-05, 2.99462090e-06,\n",
       "       1.83076918e-06, 5.66196604e-06, 1.42336103e-06, 1.53588803e-06,\n",
       "       7.88125988e-07, 1.25118015e-06, 1.06523192e-04, 5.12877614e-06,\n",
       "       7.62311913e-07, 3.06260446e-03, 1.83443790e-05, 8.84590008e-06,\n",
       "       1.61100661e-06, 2.95046284e-05, 7.88832210e-07, 8.70668032e-07,\n",
       "       7.91002458e-06, 9.13726069e-07, 1.91046411e-06, 1.61990363e-06,\n",
       "       1.76112717e-05, 1.49922539e-06, 9.99887466e-01, 1.33657258e-03,\n",
       "       3.17685249e-06, 1.27361579e-06, 1.56386122e-05, 9.12192045e-04,\n",
       "       2.00052236e-05, 2.19437352e-05, 2.86882860e-05, 1.42663732e-04,\n",
       "       9.99921799e-01, 2.00337922e-06, 8.75483863e-07, 9.98623252e-01,\n",
       "       1.23758639e-06, 2.67072664e-05, 3.84770829e-06, 5.31815522e-06,\n",
       "       9.99625802e-01, 3.06366633e-06, 2.62234607e-06, 3.71710553e-06,\n",
       "       1.41313308e-06, 1.13972987e-06, 5.52312296e-04, 1.07999392e-06,\n",
       "       1.46139018e-06, 7.89319802e-06, 3.72390377e-06, 6.47619163e-06,\n",
       "       9.48037177e-06, 1.54725547e-06, 2.69656739e-05, 3.81458085e-06,\n",
       "       9.99881029e-01, 1.08632230e-05, 1.06653606e-05, 5.90475747e-06,\n",
       "       9.99948740e-01, 1.11352665e-06, 8.08672382e-07, 4.01924808e-06,\n",
       "       3.77689048e-06, 3.33950720e-06, 9.99829173e-01, 3.33077628e-06,\n",
       "       9.04124477e-07, 5.18717388e-06, 4.03834292e-06, 7.45759507e-07,\n",
       "       1.33523679e-06, 2.48556921e-06, 1.23326327e-05, 1.35212713e-06,\n",
       "       8.59322427e-06, 1.08466668e-06, 9.03138584e-07, 1.05562867e-05,\n",
       "       4.70522900e-06, 5.54801445e-05, 4.46279300e-05, 3.37049482e-06,\n",
       "       1.20044240e-06, 2.43852878e-06, 1.36648794e-06, 1.42389047e-06,\n",
       "       2.03650288e-05, 1.21775111e-06, 2.32754082e-05, 3.48665926e-05,\n",
       "       1.90589762e-05, 2.38230386e-05, 6.85444138e-06, 1.01128389e-05,\n",
       "       3.31712340e-06, 6.84844281e-06, 1.48625577e-05, 1.62789038e-05,\n",
       "       3.53989594e-06, 5.84220243e-06, 1.52979101e-06, 8.29195130e-07,\n",
       "       5.95737129e-06, 8.59793818e-06, 5.20139838e-05, 2.87427724e-06,\n",
       "       4.47512502e-06, 7.31581895e-06, 9.99859929e-01, 7.88000580e-06,\n",
       "       1.10081210e-05, 3.06612958e-04, 3.76772041e-05, 1.62400886e-06,\n",
       "       1.89737659e-06, 2.10236021e-06, 9.99913335e-01, 1.09136356e-02,\n",
       "       1.02238901e-05, 3.21813422e-06, 2.51529389e-03, 9.99512553e-01,\n",
       "       1.59175045e-06, 8.60361979e-07, 1.85374229e-05, 1.59524120e-06,\n",
       "       3.22736087e-05, 1.98630965e-03, 2.79984965e-06, 1.40320219e-06,\n",
       "       2.76319565e-06, 3.24867710e-06, 1.15971568e-06, 1.96131509e-06,\n",
       "       1.27897124e-06, 1.16612537e-05, 5.69123622e-05, 2.35908738e-06,\n",
       "       2.37355725e-05, 9.99894619e-01, 3.64122570e-05, 1.81467294e-06,\n",
       "       3.45918716e-06, 9.97204734e-06, 2.62942058e-06, 1.28667789e-05,\n",
       "       3.59163437e-06, 3.97127842e-05, 4.64930199e-05, 2.19445337e-06,\n",
       "       1.41672922e-06, 1.36507981e-06, 1.27777412e-06, 9.99946833e-01,\n",
       "       5.83672227e-06, 4.16851362e-05, 7.48567936e-06, 1.05493166e-06,\n",
       "       1.41743055e-06, 1.31293223e-06, 1.05830532e-06, 1.69984341e-05,\n",
       "       3.75272102e-05, 3.52362099e-06, 2.00230315e-05, 9.65815707e-06,\n",
       "       1.53332553e-06, 9.99915123e-01, 1.42939671e-05, 1.90466665e-06,\n",
       "       9.99726593e-01, 7.91317120e-07, 1.17947127e-06, 2.41436283e-05,\n",
       "       1.54381814e-06, 9.99897599e-01, 2.49676341e-05, 1.78826576e-06,\n",
       "       2.96686805e-04, 2.50371909e-06, 1.70305793e-05, 1.02093827e-05,\n",
       "       1.79915878e-06, 3.37417418e-06, 2.69997940e-06, 4.25730514e-06,\n",
       "       9.72892849e-07, 2.11318479e-06, 7.87025056e-06, 3.75441391e-06,\n",
       "       3.40360975e-06, 9.52398182e-07, 8.97994141e-06, 9.99935150e-01,\n",
       "       1.74261606e-06, 9.66669518e-07, 8.97527570e-06, 2.69425927e-05,\n",
       "       3.68965311e-06, 1.06286393e-06, 1.08194481e-05, 9.06065679e-07,\n",
       "       1.33766855e-06, 1.34924562e-06, 2.06490245e-06, 1.88446211e-04,\n",
       "       1.04398932e-05, 2.08210804e-06, 9.99931574e-01, 4.80731478e-06,\n",
       "       9.80589789e-07, 1.13532456e-04, 1.01439018e-05, 4.84466318e-06,\n",
       "       1.49305615e-05, 3.03400284e-05, 9.99890208e-01, 1.56762144e-06,\n",
       "       1.16761271e-06, 3.65338237e-05, 1.38533087e-05, 4.21976983e-06,\n",
       "       1.70168357e-06, 1.72714988e-06, 3.97451777e-06, 6.57726650e-06,\n",
       "       4.47497996e-06, 1.02877880e-06, 8.84551264e-05, 9.99332130e-01,\n",
       "       1.87550575e-06, 1.00523866e-05, 4.86134331e-06, 9.99880314e-01,\n",
       "       1.57708166e-06, 1.33016181e-06, 7.08546395e-06, 1.39958831e-06,\n",
       "       2.02814190e-05, 4.25999360e-06, 1.51379788e-06, 1.01307637e-06,\n",
       "       1.37477318e-05, 2.16601825e-06, 1.33098394e-04, 3.65259439e-06,\n",
       "       6.15669114e-06, 2.90488993e-06, 3.99727924e-06, 1.70468218e-06,\n",
       "       9.55293672e-07, 9.46829516e-07, 1.78982168e-06, 2.37937934e-06,\n",
       "       2.78482526e-06, 9.91445995e-06, 1.13549231e-06, 1.11531745e-06,\n",
       "       3.20786899e-06, 6.72526937e-07, 1.25536462e-06, 9.99113262e-01,\n",
       "       2.82344558e-06, 9.25198674e-06, 1.81369705e-06, 3.36177277e-06,\n",
       "       9.21916239e-07, 1.53174699e-06, 2.82828296e-06, 2.05365377e-05,\n",
       "       9.99882221e-01, 6.09132314e-07, 1.73376939e-05, 3.58571015e-06,\n",
       "       1.07134110e-06, 9.99898553e-01, 3.42705425e-06, 9.28198403e-07,\n",
       "       1.13038259e-05, 1.18060120e-06, 3.62302626e-06, 3.33361750e-05,\n",
       "       9.16081888e-07, 1.87139119e-06, 2.65180529e-06, 4.49936306e-06,\n",
       "       1.16956073e-06, 8.24353094e-07, 1.35581024e-04, 2.90147636e-06,\n",
       "       2.46769723e-05, 1.39663371e-06, 4.47525281e-06, 7.07918934e-06,\n",
       "       1.62759670e-06, 1.20648735e-06, 1.70948954e-06, 1.16000990e-06,\n",
       "       2.51634301e-05, 1.76592221e-06, 9.02779561e-07, 7.22174093e-07,\n",
       "       1.02380682e-06, 5.62287869e-06, 2.11130919e-06, 9.99912739e-01,\n",
       "       1.85353729e-05, 1.42285528e-04, 9.17675789e-06, 1.33248659e-06,\n",
       "       9.26610664e-06, 8.36357117e-07, 6.12516524e-05, 1.90655937e-05,\n",
       "       1.42023291e-05, 8.42437385e-06, 1.00399507e-06, 1.16170600e-06,\n",
       "       9.66518314e-07, 3.03200995e-05, 7.71119176e-06, 7.24009578e-06,\n",
       "       1.19009937e-05, 4.43603312e-06, 1.41605951e-05, 3.29630075e-06,\n",
       "       7.47189688e-07, 9.99892354e-01, 1.65444817e-06, 9.13586609e-06,\n",
       "       2.38105235e-06, 2.06514087e-05, 6.56826887e-05, 2.20351717e-06,\n",
       "       1.20963159e-05, 2.42724927e-05, 1.05799347e-06, 1.68890176e-06,\n",
       "       9.54621555e-07, 1.45296724e-06, 2.82278191e-03, 1.27353378e-05,\n",
       "       1.25373981e-06, 1.65830261e-06, 1.33871390e-06, 1.12641501e-05,\n",
       "       9.22819652e-07, 2.33489518e-05, 7.70788233e-07, 1.67433666e-06,\n",
       "       8.98112319e-07, 1.10301780e-05, 3.29600198e-06, 1.28558452e-06,\n",
       "       2.42688452e-06, 6.79657678e-05, 6.38665051e-06, 7.96558879e-07,\n",
       "       3.78882464e-06, 3.69102090e-05, 1.80658571e-05, 2.05970423e-06,\n",
       "       1.71566319e-06, 1.30204182e-06, 9.99909163e-01, 1.01545879e-06,\n",
       "       9.99910593e-01, 1.49554239e-06, 1.33686126e-06, 1.27711496e-06,\n",
       "       9.25638403e-07, 1.86008547e-05, 1.27209466e-06, 8.84967176e-06,\n",
       "       4.82991027e-06, 3.80152892e-06, 1.50262622e-06, 9.99209285e-01,\n",
       "       1.01486721e-05, 1.44032856e-05, 7.10130109e-07, 2.33604442e-05,\n",
       "       1.02501133e-06, 5.94852354e-05, 7.05095154e-05, 1.02045833e-05,\n",
       "       4.47414895e-05, 5.01077147e-06, 9.87815383e-06, 1.21274916e-05,\n",
       "       1.59414014e-06, 7.48725483e-07, 1.32395678e-06, 1.45097456e-05,\n",
       "       2.37347331e-06, 1.32548030e-06, 1.64226443e-03, 9.56559916e-07,\n",
       "       9.32497983e-07, 3.89142269e-06, 8.57011742e-07, 4.41485099e-06,\n",
       "       8.44185888e-06, 8.98814960e-06, 1.36032559e-05, 4.57165606e-06,\n",
       "       5.06461656e-05, 1.81941050e-06, 8.61417732e-07, 1.42852459e-05,\n",
       "       9.99928713e-01, 9.99313712e-01, 8.42659938e-06, 9.99865532e-01,\n",
       "       3.89288152e-06, 2.66359893e-06, 1.91016170e-06, 1.75339835e-06,\n",
       "       5.91110711e-06, 2.33326909e-06, 3.28943929e-06, 3.81101381e-06,\n",
       "       9.39619042e-07, 3.17248055e-06, 2.42701196e-03, 1.86683916e-04,\n",
       "       1.29508317e-06, 1.85849858e-05, 9.65628374e-07, 9.99938369e-01,\n",
       "       9.99817193e-01, 1.60335378e-06, 6.99164957e-05, 1.67324779e-05,\n",
       "       9.14214240e-07, 1.47204310e-06, 9.39116944e-06, 1.60884847e-05,\n",
       "       1.82863516e-06, 7.41188487e-05, 2.99012049e-06, 5.34551218e-05,\n",
       "       6.64319896e-06, 8.16671218e-06, 1.07745302e-06, 1.28220681e-05,\n",
       "       3.26971031e-06, 1.23973973e-06, 1.47609876e-06, 9.98290837e-01,\n",
       "       8.49643311e-06, 8.49439516e-07, 6.82377413e-06, 9.99867439e-01,\n",
       "       1.04973913e-06, 1.49198586e-05, 1.39878612e-04, 2.22779317e-06,\n",
       "       3.35328082e-06, 7.84623171e-06, 1.25205145e-06, 6.71910948e-06,\n",
       "       1.92701145e-06, 1.29535363e-06, 7.86281635e-06, 9.99915957e-01,\n",
       "       4.37227209e-06, 9.98242259e-01, 8.32182479e-07, 8.33682043e-06,\n",
       "       9.99810874e-01, 8.76139495e-07, 7.18194497e-06, 1.23564860e-06,\n",
       "       3.41375858e-06, 9.99926567e-01, 6.26913679e-05, 9.99917865e-01,\n",
       "       1.47423896e-06, 9.99935031e-01, 9.99889851e-01, 9.99432266e-01,\n",
       "       9.98824060e-01, 9.99919891e-01, 9.99870777e-01, 9.99862432e-01,\n",
       "       9.99852777e-01, 9.99906898e-01, 9.99782383e-01, 9.99739707e-01,\n",
       "       9.99852061e-01, 9.99914289e-01, 9.99902487e-01, 9.99882579e-01,\n",
       "       9.99639988e-01, 9.99880075e-01, 9.99868393e-01, 9.99852061e-01,\n",
       "       9.99901414e-01, 9.99928832e-01, 9.99873877e-01, 9.99320865e-01,\n",
       "       9.99682784e-01, 9.99808371e-01, 9.99853849e-01, 9.99840498e-01,\n",
       "       9.99940276e-01, 9.99824584e-01, 9.99837518e-01, 9.99907613e-01,\n",
       "       9.99877334e-01, 9.99917507e-01, 9.99815643e-01, 9.99859333e-01,\n",
       "       9.99890566e-01, 9.99860168e-01, 9.99815285e-01, 9.99851942e-01,\n",
       "       9.68083262e-01, 9.99888062e-01, 9.99945641e-01, 9.99743640e-01,\n",
       "       9.99886870e-01, 9.07261431e-01, 9.98156726e-01, 5.37234426e-01,\n",
       "       7.23813951e-01, 9.99916315e-01, 9.99890804e-01, 9.99854803e-01,\n",
       "       8.05122614e-01, 9.99851942e-01, 9.99858379e-01, 9.99895692e-01,\n",
       "       9.99781311e-01, 9.99832273e-01, 9.99897599e-01, 9.99619246e-01,\n",
       "       9.99291658e-01, 9.99674439e-01, 9.99876618e-01, 9.99706924e-01,\n",
       "       6.18722856e-01, 9.99580920e-01, 6.09173000e-01, 9.99524593e-01,\n",
       "       9.52830672e-01, 9.99755561e-01, 1.29439592e-01, 9.78131354e-01,\n",
       "       8.04904997e-01, 9.56045628e-01, 9.99835730e-01, 9.99317169e-01,\n",
       "       9.90927339e-01], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_value = np.concatenate((data_dg, check))\n",
    "lable_value = np.concatenate((label_dg, np.ones(check.shape[0])))\n",
    "\n",
    "predict = model_antonio_reloaded.predict(data_value).ravel()\n",
    "\n",
    "#classification_report(lable_value, predict)\n",
    "#print(model_giovanni.evaluate(data_dg, label_dg))\n",
    "#print(model_manuel.evaluate(data_dg, label_dg))\n",
    "#print(model_antonio_reloaded.evaluate(data_dg, label_dg))\n",
    "#attack\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "target_names = ['rejection', 'acceptance']\n",
    "rep = classification_report(lable_value, predict,target_names=target_names)\n",
    "#print(rep)\n",
    "roc_auc_score(lable_value,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "roc_auc = metrics.auc(fpr_keras, tpr_keras)\n",
    "\n",
    "plt.plot(fpr_keras,tpr_keras,'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.title('ROC curve')\n",
    "plt.ylabel('tpr')\n",
    "plt.xlabel('fpr')\n",
    "plt.legend(['curve'], loc='lower right')\n",
    "plt.savefig('ROC_curve_Antonio_Ultima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
